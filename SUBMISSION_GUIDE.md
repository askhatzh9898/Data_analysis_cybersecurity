# Academic Submission Guide

## Cybersecurity Network Data Analysis Pipeline

This document provides guidance for academic evaluation of the cybersecurity data analysis pipeline. The project demonstrates end-to-end processing of network telemetry from raw PCAP-derived Zeek logs to enriched, analysis-ready datasets.

---

## Quick Start (Reproducibility Check)

```bash
# 1. Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run the complete pipeline
python scripts/pipeline.py

# 4. Verify outputs exist
ls -la outputs/final/
```

Expected output files:
- `outputs/final/enriched_network_events.csv` — Main analysis dataset
- `outputs/final/investigation_priorities.csv` — Ranked suspicious hosts
- `outputs/final/analysis_summary.json` — Statistical findings

---

## Project Structure

```
data_analysis/
│
├── data/                          # DATA LAYER
│   ├── raw_pcaps/                 # [RAW] Original packet captures
│   ├── zeek_logs/                 # [RAW] Zeek-generated logs from PCAPs
│   │   └── sample/                # Sample dataset for testing
│   │       ├── conn.log           # Connection records (JSON)
│   │       └── dns.log            # DNS queries (JSON)
│   ├── geoip/                     # [REFERENCE] MaxMind databases (optional)
│   └── ti/                        # [REFERENCE] Threat intelligence feeds
│       ├── sample_ips.txt         # IP blacklist
│       └── sample_domains.txt     # Domain blacklist
│
├── outputs/                       # OUTPUT LAYER (generated by pipeline)
│   ├── intermediate/              # [INTERMEDIATE] Stage outputs (Parquet)
│   │   ├── 01_conn_raw.parquet
│   │   ├── 01_dns_raw.parquet
│   │   ├── 02_conn_normalized.parquet
│   │   ├── 02_dns_normalized.parquet
│   │   └── 03_unified_enriched.parquet
│   ├── final/                     # [FINAL] Deliverable outputs (CSV/JSON)
│   │   ├── enriched_network_events.csv
│   │   ├── investigation_priorities.csv
│   │   └── analysis_summary.json
│   └── metadata/                  # [METADATA] Reproducibility artifacts
│       ├── pipeline_manifest.json
│       └── data_checksums.json
│
├── scripts/                       # CODE LAYER
│   ├── pipeline.py                # Main orchestration script
│   ├── zeek_to_dataframe.py       # Zeek log parser
│   ├── normalize.py               # Schema normalization
│   ├── enrich_ti.py               # Threat intelligence enrichment
│   ├── enrich_geoip.py            # GeoIP/ASN enrichment (optional)
│   └── security_eda.py            # Statistical analysis
│
├── notebooks/                     # EXPLORATION LAYER
│   ├── 01_conn_analysis.ipynb     # Data loading walkthrough
│   ├── 02_normalized_schema.ipynb # Normalization demonstration
│   └── 03_security_eda.ipynb      # Interactive analysis
│
├── requirements.txt               # Python dependencies
├── README.md                      # Project overview
└── SUBMISSION_GUIDE.md            # This file
```

---

## Data Separation Philosophy

### Raw Data (`data/`)
- **Never modified** by the pipeline
- Contains original Zeek logs as produced by `zeek -r capture.pcap`
- Threat intelligence files are version-controlled reference data

### Intermediate Data (`outputs/intermediate/`)
- Parquet format for efficient storage and fast reloading
- Each file corresponds to a pipeline stage
- Useful for debugging and partial re-runs

### Final Outputs (`outputs/final/`)
- CSV format for maximum portability and human readability
- JSON for structured metadata
- These are the **graded deliverables**

### Metadata (`outputs/metadata/`)
- `pipeline_manifest.json` — Execution parameters, timestamps, schema
- `data_checksums.json` — SHA-256 hashes of all output files
- Enables **exact reproducibility verification**

---

## Pipeline Stages

| Stage | Input | Output | Purpose |
|-------|-------|--------|---------|
| 1. LOAD | Zeek JSON logs | Raw DataFrames | Parse Zeek format, apply schemas |
| 2. NORMALIZE | Raw DataFrames | Unified schema | Rename fields (id.orig_h → src_ip), ensure consistent dtypes |
| 3. ENRICH | Normalized data | Enriched data | Add ti_match boolean from threat intel |
| 4. ANALYZE | Enriched data | Findings object | Compute statistics, detect anomalies |
| 5. EXPORT | All stages | CSV/JSON files | Save final outputs with metadata |

---

## Reproducibility Verification

### Method 1: Checksum Comparison
```bash
# Run pipeline twice
python scripts/pipeline.py --output-dir outputs/run1
python scripts/pipeline.py --output-dir outputs/run2

# Compare checksums
diff outputs/run1/metadata/data_checksums.json outputs/run2/metadata/data_checksums.json
```

If the pipeline is deterministic, checksums will match exactly.

### Method 2: Row Count Verification
```python
import pandas as pd

df = pd.read_csv("outputs/final/enriched_network_events.csv")
print(f"Rows: {len(df)}")        # Expected: 21
print(f"Columns: {len(df.columns)}")  # Expected: 20
print(f"TI matches: {df['ti_match'].sum()}")  # Expected: 6
```

### Method 3: Manifest Inspection
```bash
cat outputs/metadata/pipeline_manifest.json
```

Shows exact execution parameters, Python version, and schema used.

---

## Evaluation Criteria

### Correctness (40%)
- [ ] Pipeline runs without errors
- [ ] Output files are valid CSV/JSON
- [ ] Row counts match expected values
- [ ] TI matches are correctly identified

### Reproducibility (30%)
- [ ] Same input produces identical checksums
- [ ] Dependencies are pinned in requirements.txt
- [ ] No hardcoded absolute paths
- [ ] Random seeds set (if applicable)

### Code Quality (20%)
- [ ] Clear separation of concerns (one module per task)
- [ ] Consistent naming conventions
- [ ] Docstrings on public functions
- [ ] Type hints used where appropriate

### Documentation (10%)
- [ ] README explains project purpose
- [ ] This guide enables independent reproduction
- [ ] Notebooks provide exploratory context

---

## Grading Checklist

For evaluators to verify a submission:

```bash
# 1. Fresh environment
python3 -m venv test_env
source test_env/bin/activate
pip install -r requirements.txt

# 2. Run pipeline
python scripts/pipeline.py

# 3. Verify outputs
test -f outputs/final/enriched_network_events.csv && echo "✓ CSV exists"
test -f outputs/final/analysis_summary.json && echo "✓ JSON exists"
test -f outputs/metadata/data_checksums.json && echo "✓ Checksums exist"

# 4. Verify row count
python -c "import pandas as pd; df=pd.read_csv('outputs/final/enriched_network_events.csv'); print(f'Rows: {len(df)}')"

# 5. Verify TI matches
python -c "import pandas as pd; df=pd.read_csv('outputs/final/enriched_network_events.csv'); print(f'TI matches: {df[\"ti_match\"].sum()}')"
```

---

## Known Limitations

1. **GeoIP enrichment disabled by default** — Requires MaxMind database download
2. **Sample dataset is small** — 12 connections, 9 DNS queries (for fast testing)
3. **No real-time processing** — Designed for offline PCAP analysis only

---

## Contact

For questions about this submission, refer to the inline documentation in each script or the notebooks for interactive exploration.
