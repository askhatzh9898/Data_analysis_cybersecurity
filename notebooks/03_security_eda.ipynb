{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Security Exploratory Data Analysis\n",
    "\n",
    "Statistical analysis of network telemetry to surface suspicious patterns.\n",
    "No machine learning—focused on interpretable metrics that analysts can act on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scripts.zeek_to_dataframe import load_zeek_log, CONN_SCHEMA, DNS_SCHEMA\n",
    "from scripts.normalize import normalize_conn, normalize_dns, merge_normalized\n",
    "from scripts.enrich_ti import ThreatIntel, enrich_ti\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 — Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZEEK_DIR = Path(\"../data/zeek_logs/sample\")\n",
    "\n",
    "# Load and normalize\n",
    "conn_raw = load_zeek_log(ZEEK_DIR / \"conn.log\", schema=CONN_SCHEMA)\n",
    "dns_raw = load_zeek_log(ZEEK_DIR / \"dns.log\", schema=DNS_SCHEMA)\n",
    "\n",
    "conn = normalize_conn(conn_raw)\n",
    "dns = normalize_dns(dns_raw)\n",
    "\n",
    "# Load TI and enrich\n",
    "ti = ThreatIntel()\n",
    "ti.load_ip_blacklist(\"../data/ti/sample_ips.txt\")\n",
    "ti.load_domain_blacklist(\"../data/ti/sample_domains.txt\")\n",
    "\n",
    "conn = enrich_ti(conn, ti, ip_column=\"dst_ip\", domain_column=None)\n",
    "dns = enrich_ti(dns, ti, ip_column=\"dst_ip\", domain_column=\"dns_query\")\n",
    "\n",
    "# Add mock country data for demonstration\n",
    "# In production, use: from scripts.enrich_geoip import enrich_geoip\n",
    "country_map = {\n",
    "    \"93.184.216.34\": \"US\",    # example.com\n",
    "    \"8.8.8.8\": \"US\",          # Google DNS\n",
    "    \"8.8.4.4\": \"US\",          # Google DNS\n",
    "    \"185.199.108.153\": \"US\",  # GitHub\n",
    "    \"44.230.90.12\": \"US\",     # AWS (suspicious)\n",
    "    \"44.230.90.13\": \"US\",     # AWS (suspicious)\n",
    "    \"44.230.90.14\": \"US\",     # AWS (suspicious)\n",
    "    \"151.101.1.140\": \"US\",    # Reddit/Fastly\n",
    "    \"172.217.14.99\": \"US\",    # Google\n",
    "}\n",
    "conn[\"dst_country\"] = conn[\"dst_ip\"].map(country_map).astype(\"string\")\n",
    "dns[\"dst_country\"] = dns[\"dst_ip\"].map(country_map).astype(\"string\")\n",
    "\n",
    "# Merge into unified timeline\n",
    "unified = merge_normalized(conn, dns)\n",
    "\n",
    "print(f\"Connections: {len(conn)}\")\n",
    "print(f\"DNS queries: {len(dns)}\")\n",
    "print(f\"Unified events: {len(unified)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 — Traffic Distribution by Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protocol breakdown\n",
    "protocol_counts = conn[\"protocol\"].value_counts()\n",
    "protocol_bytes = conn.groupby(\"protocol\")[[\"bytes_sent\", \"bytes_recv\"]].sum()\n",
    "protocol_bytes[\"total_bytes\"] = protocol_bytes[\"bytes_sent\"].fillna(0) + protocol_bytes[\"bytes_recv\"].fillna(0)\n",
    "\n",
    "print(\"=== Protocol Distribution (Connection Count) ===\")\n",
    "print(protocol_counts.to_string())\n",
    "print()\n",
    "print(\"=== Protocol Distribution (Bytes Transferred) ===\")\n",
    "print(protocol_bytes.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Connection count\n",
    "protocol_counts.plot.pie(ax=axes[0], autopct=\"%1.0f%%\", startangle=90)\n",
    "axes[0].set_title(\"Connections by Protocol\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "# Bytes transferred\n",
    "protocol_bytes[\"total_bytes\"].plot.pie(ax=axes[1], autopct=\"%1.0f%%\", startangle=90)\n",
    "axes[1].set_title(\"Bytes by Protocol\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service breakdown within TCP\n",
    "tcp_services = conn[conn[\"protocol\"] == \"tcp\"][\"service\"].value_counts(dropna=False)\n",
    "print(\"=== TCP Services ===\")\n",
    "print(tcp_services.to_string())\n",
    "print()\n",
    "print(\"Note: <NA> indicates connections where Zeek couldn't identify the application protocol\")\n",
    "print(\"      (often failed connections or encrypted traffic without SNI)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 — Top Destination Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country distribution (external IPs only)\n",
    "external_conn = conn[conn[\"dst_country\"].notna()]\n",
    "\n",
    "country_stats = external_conn.groupby(\"dst_country\").agg(\n",
    "    connections=(\"uid\", \"count\"),\n",
    "    unique_dst_ips=(\"dst_ip\", \"nunique\"),\n",
    "    bytes_sent=(\"bytes_sent\", \"sum\"),\n",
    "    bytes_recv=(\"bytes_recv\", \"sum\"),\n",
    ").sort_values(\"connections\", ascending=False)\n",
    "\n",
    "country_stats[\"bytes_sent\"] = country_stats[\"bytes_sent\"].fillna(0).astype(int)\n",
    "country_stats[\"bytes_recv\"] = country_stats[\"bytes_recv\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"=== Traffic by Destination Country ===\")\n",
    "print(country_stats.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "if len(country_stats) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    country_stats[\"connections\"].head(10).plot.barh(ax=ax, color=\"steelblue\")\n",
    "    ax.set_xlabel(\"Connection Count\")\n",
    "    ax.set_title(\"Top 10 Destination Countries\")\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 — Suspicious Pattern Detection (Statistics-Based)\n",
    "\n",
    "Simple statistical methods to surface anomalies without machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 — Connection State Analysis (Failed Connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection state distribution\n",
    "# S0 = SYN sent, no response (potential scanning or blocked traffic)\n",
    "# SF = Normal completion\n",
    "# REJ = Connection rejected\n",
    "# RSTO/RSTR = Reset by originator/responder\n",
    "\n",
    "state_counts = conn[\"conn_state\"].value_counts()\n",
    "state_pct = (state_counts / len(conn) * 100).round(1)\n",
    "\n",
    "state_analysis = pd.DataFrame({\n",
    "    \"count\": state_counts,\n",
    "    \"percent\": state_pct,\n",
    "})\n",
    "\n",
    "print(\"=== Connection State Distribution ===\")\n",
    "print(state_analysis.to_string())\n",
    "print()\n",
    "\n",
    "# Flag: High S0 rate indicates scanning or firewall blocks\n",
    "s0_count = state_counts.get(\"S0\", 0)\n",
    "s0_rate = s0_count / len(conn) * 100\n",
    "if s0_rate > 10:\n",
    "    print(f\"⚠️  ALERT: {s0_rate:.1f}% S0 (no response) — potential port scanning or blocked egress\")\n",
    "elif s0_rate > 5:\n",
    "    print(f\"⚡ WARNING: {s0_rate:.1f}% S0 — elevated failed connections\")\n",
    "else:\n",
    "    print(f\"✓ S0 rate ({s0_rate:.1f}%) within normal range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which source IPs have the most failed connections?\n",
    "failed_states = [\"S0\", \"REJ\", \"RSTO\", \"RSTR\", \"S1\", \"S2\", \"S3\"]\n",
    "failed_conn = conn[conn[\"conn_state\"].isin(failed_states)]\n",
    "\n",
    "if len(failed_conn) > 0:\n",
    "    failed_by_src = failed_conn.groupby(\"src_ip\").agg(\n",
    "        failed_count=(\"uid\", \"count\"),\n",
    "        unique_dst_ips=(\"dst_ip\", \"nunique\"),\n",
    "        unique_dst_ports=(\"dst_port\", \"nunique\"),\n",
    "    ).sort_values(\"failed_count\", ascending=False)\n",
    "    \n",
    "    print(\"=== Failed Connections by Source IP ===\")\n",
    "    print(failed_by_src.to_string())\n",
    "    print()\n",
    "    \n",
    "    # Flag: Many unique dst_ports from one source = likely scanning\n",
    "    scanners = failed_by_src[failed_by_src[\"unique_dst_ports\"] > 3]\n",
    "    if len(scanners) > 0:\n",
    "        print(\"⚠️  Potential scanners (failed connections to many ports):\")\n",
    "        for ip in scanners.index:\n",
    "            print(f\"    {ip}: {scanners.loc[ip, 'unique_dst_ports']} unique ports\")\n",
    "else:\n",
    "    print(\"✓ No failed connections detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 — DNS Anomaly Detection (NXDOMAIN Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNS response code distribution\n",
    "rcode_counts = dns[\"dns_rcode\"].value_counts()\n",
    "rcode_pct = (rcode_counts / len(dns) * 100).round(1)\n",
    "\n",
    "rcode_analysis = pd.DataFrame({\n",
    "    \"count\": rcode_counts,\n",
    "    \"percent\": rcode_pct,\n",
    "})\n",
    "\n",
    "print(\"=== DNS Response Code Distribution ===\")\n",
    "print(rcode_analysis.to_string())\n",
    "print()\n",
    "\n",
    "# Flag: High NXDOMAIN rate may indicate DGA malware\n",
    "nxdomain_count = rcode_counts.get(\"NXDOMAIN\", 0)\n",
    "nxdomain_rate = nxdomain_count / len(dns) * 100\n",
    "if nxdomain_rate > 30:\n",
    "    print(f\"⚠️  ALERT: {nxdomain_rate:.1f}% NXDOMAIN — potential DGA activity\")\n",
    "elif nxdomain_rate > 15:\n",
    "    print(f\"⚡ WARNING: {nxdomain_rate:.1f}% NXDOMAIN — elevated lookup failures\")\n",
    "else:\n",
    "    print(f\"✓ NXDOMAIN rate ({nxdomain_rate:.1f}%) within normal range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which hosts are generating NXDOMAIN responses?\n",
    "nxdomain_queries = dns[dns[\"dns_rcode\"] == \"NXDOMAIN\"]\n",
    "\n",
    "if len(nxdomain_queries) > 0:\n",
    "    nx_by_src = nxdomain_queries.groupby(\"src_ip\").agg(\n",
    "        nxdomain_count=(\"uid\", \"count\"),\n",
    "        unique_domains=(\"dns_query\", \"nunique\"),\n",
    "    ).sort_values(\"nxdomain_count\", ascending=False)\n",
    "    \n",
    "    print(\"=== NXDOMAIN by Source IP ===\")\n",
    "    print(nx_by_src.to_string())\n",
    "    print()\n",
    "    \n",
    "    # Show the actual failed domains\n",
    "    print(\"=== Failed Domain Lookups ===\")\n",
    "    print(nxdomain_queries[[\"timestamp\", \"src_ip\", \"dns_query\"]].to_string())\n",
    "else:\n",
    "    print(\"✓ No NXDOMAIN responses detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 — Data Transfer Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bytes ratio: bytes_sent / bytes_recv\n",
    "# High ratio (>> 1) = uploading data (potential exfiltration)\n",
    "# Low ratio (<< 1) = downloading data (normal browsing)\n",
    "\n",
    "conn_with_bytes = conn[\n",
    "    (conn[\"bytes_sent\"].notna()) & \n",
    "    (conn[\"bytes_recv\"].notna()) &\n",
    "    (conn[\"bytes_recv\"] > 0)\n",
    "].copy()\n",
    "\n",
    "if len(conn_with_bytes) > 0:\n",
    "    conn_with_bytes[\"bytes_ratio\"] = conn_with_bytes[\"bytes_sent\"] / conn_with_bytes[\"bytes_recv\"]\n",
    "    \n",
    "    print(\"=== Data Transfer Statistics ===\")\n",
    "    print(f\"Mean bytes_sent:  {conn_with_bytes['bytes_sent'].mean():,.0f}\")\n",
    "    print(f\"Mean bytes_recv:  {conn_with_bytes['bytes_recv'].mean():,.0f}\")\n",
    "    print(f\"Median bytes_ratio: {conn_with_bytes['bytes_ratio'].median():.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # Flag connections with unusually high upload ratio\n",
    "    high_upload = conn_with_bytes[conn_with_bytes[\"bytes_ratio\"] > 1.0]\n",
    "    if len(high_upload) > 0:\n",
    "        print(\"=== High Upload Ratio Connections (potential exfiltration) ===\")\n",
    "        print(high_upload[[\"timestamp\", \"src_ip\", \"dst_ip\", \"dst_port\", \"bytes_sent\", \"bytes_recv\", \"bytes_ratio\"]].to_string())\n",
    "    else:\n",
    "        print(\"✓ No high upload ratio connections detected\")\n",
    "else:\n",
    "    print(\"Insufficient data for bytes analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top data senders (by bytes_sent to external IPs)\n",
    "external_transfers = conn[conn[\"dst_country\"].notna()].copy()\n",
    "\n",
    "if len(external_transfers) > 0:\n",
    "    top_senders = external_transfers.groupby(\"src_ip\").agg(\n",
    "        total_bytes_sent=(\"bytes_sent\", \"sum\"),\n",
    "        connection_count=(\"uid\", \"count\"),\n",
    "        unique_destinations=(\"dst_ip\", \"nunique\"),\n",
    "    ).sort_values(\"total_bytes_sent\", ascending=False)\n",
    "    \n",
    "    top_senders[\"total_bytes_sent\"] = top_senders[\"total_bytes_sent\"].fillna(0).astype(int)\n",
    "    \n",
    "    print(\"=== Top Data Senders (to external IPs) ===\")\n",
    "    print(top_senders.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 — Threat Intelligence Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TI match summary\n",
    "ti_matches = unified[unified[\"ti_match\"] == True]\n",
    "\n",
    "print(\"=== Threat Intelligence Matches ===\")\n",
    "print(f\"Total events: {len(unified)}\")\n",
    "print(f\"TI matches: {len(ti_matches)} ({len(ti_matches)/len(unified)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "if len(ti_matches) > 0:\n",
    "    print(\"=== TI Matches by Log Type ===\")\n",
    "    print(ti_matches[\"log_type\"].value_counts().to_string())\n",
    "    print()\n",
    "    \n",
    "    print(\"=== Affected Source IPs ===\")\n",
    "    affected_hosts = ti_matches[\"src_ip\"].value_counts()\n",
    "    print(affected_hosts.to_string())\n",
    "    print()\n",
    "    \n",
    "    print(\"=== TI Match Details ===\")\n",
    "    print(ti_matches[[\"timestamp\", \"log_type\", \"src_ip\", \"dst_ip\", \"dst_port\", \"dns_query\"]].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 — Beaconing Detection (Connection Timing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple beaconing detection: look for repeated connections to same destination\n",
    "# with regular intervals (C2 callback behavior)\n",
    "\n",
    "conn_pairs = conn.groupby([\"src_ip\", \"dst_ip\", \"dst_port\"]).agg(\n",
    "    connection_count=(\"uid\", \"count\"),\n",
    "    first_seen=(\"timestamp\", \"min\"),\n",
    "    last_seen=(\"timestamp\", \"max\"),\n",
    ").reset_index()\n",
    "\n",
    "# Flag: Multiple connections to same destination = potential beaconing\n",
    "repeated = conn_pairs[conn_pairs[\"connection_count\"] > 1].sort_values(\"connection_count\", ascending=False)\n",
    "\n",
    "print(\"=== Repeated Connection Patterns ===\")\n",
    "if len(repeated) > 0:\n",
    "    print(repeated.to_string())\n",
    "    print()\n",
    "    print(\"Note: Repeated connections to same dst_ip:dst_port may indicate:\")\n",
    "    print(\"  - Normal keepalive/polling (if known service)\")\n",
    "    print(\"  - C2 beaconing (if unknown/suspicious destination)\")\n",
    "else:\n",
    "    print(\"No repeated connection patterns detected (sample too small)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 — Summary: Hosts Requiring Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all suspicious indicators per source IP\n",
    "all_src_ips = unified[\"src_ip\"].unique()\n",
    "\n",
    "investigation_list = []\n",
    "\n",
    "for src_ip in all_src_ips:\n",
    "    host_data = unified[unified[\"src_ip\"] == src_ip]\n",
    "    host_conn = conn[conn[\"src_ip\"] == src_ip]\n",
    "    host_dns = dns[dns[\"src_ip\"] == src_ip]\n",
    "    \n",
    "    flags = []\n",
    "    \n",
    "    # Check TI matches\n",
    "    ti_count = host_data[\"ti_match\"].sum()\n",
    "    if ti_count > 0:\n",
    "        flags.append(f\"TI:{ti_count}\")\n",
    "    \n",
    "    # Check failed connections\n",
    "    failed = host_conn[host_conn[\"conn_state\"].isin([\"S0\", \"REJ\"])]\n",
    "    if len(failed) > 0:\n",
    "        flags.append(f\"FAIL:{len(failed)}\")\n",
    "    \n",
    "    # Check NXDOMAIN\n",
    "    nx = host_dns[host_dns[\"dns_rcode\"] == \"NXDOMAIN\"]\n",
    "    if len(nx) > 0:\n",
    "        flags.append(f\"NX:{len(nx)}\")\n",
    "    \n",
    "    if flags:\n",
    "        investigation_list.append({\n",
    "            \"src_ip\": src_ip,\n",
    "            \"total_events\": len(host_data),\n",
    "            \"flags\": \", \".join(flags),\n",
    "            \"flag_count\": len(flags),\n",
    "        })\n",
    "\n",
    "if investigation_list:\n",
    "    invest_df = pd.DataFrame(investigation_list).sort_values(\"flag_count\", ascending=False)\n",
    "    print(\"=== HOSTS REQUIRING INVESTIGATION ===\")\n",
    "    print(invest_df.to_string(index=False))\n",
    "    print()\n",
    "    print(\"Flag legend:\")\n",
    "    print(\"  TI:N   = N threat intelligence matches\")\n",
    "    print(\"  FAIL:N = N failed connection attempts\")\n",
    "    print(\"  NX:N   = N NXDOMAIN responses (failed DNS)\")\n",
    "else:\n",
    "    print(\"✓ No hosts flagged for investigation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
